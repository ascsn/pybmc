import numpy as np


def gibbs_sampler(y, X, iterations, prior_info):
    # Make sure that "y" has the correct structure. If data is being centered, it should have the mean already substracted
    b_mean_prior, b_mean_cov, nu0, sigma20 = prior_info
    # From A_First_Course_in_Bayesian_Statistical_Methods (page ~ 159),
    # nu0 represent the effective prior samples and sigma2_0 represents the expected prior variance

    b_mean_cov_inv = np.linalg.inv(b_mean_cov)
    n = len(y)  # We are still taking mass and radius to be the same size

    X_T_X = X.T.dot(X)
    X_T_X_inv = np.linalg.inv(X_T_X)

    b_data = X_T_X_inv.dot(X.T).dot(y)

    supermodel = X.dot(b_data)

    residuals = y - supermodel

    sigma2 = np.sum(residuals**2) / len(residuals)
    cov_matrix = sigma2 * X_T_X_inv

    samples = []

    for i in range(iterations):
        # Sample from the conditional posterior of bs given sigma2 and data

        cov_matrix = np.linalg.inv(X_T_X / sigma2 + b_mean_cov_inv)

        mean_vector = cov_matrix.dot(
            b_mean_cov_inv.dot(b_mean_prior) + X.T.dot(y) / sigma2
        )

        b_current = np.random.multivariate_normal(mean_vector, cov_matrix)

        # Sample from the conditional posterior of sigma2 given bs and data
        supermodel = X.dot(b_current)

        residuals = y - supermodel

        shape_post = (nu0 + n) / 2.0
        scale_post = (nu0 * sigma20 + np.sum(residuals**2)) / 2.0
        # sigma2 = 1 / np.random.gamma(shape_post, 1/scale_post)
        sigma2 = 1 / np.random.default_rng().gamma(shape_post, 1 / scale_post)

        samples.append(np.append(b_current, np.sqrt(sigma2)))

    return np.array(samples)


def gibbs_sampler_simplex(
    y, X, Vt_hat, S_hat, iterations, prior_info, burn=10000, stepsize=0.001
):
    # y should be centered experimental data with shape (isotopes_num, 1)
    # X should be principal components kept data with shape (isotopes_num, PCs_kept)

    # This is the average weight of each model
    bias0 = np.full(len(Vt_hat.T), 1 / len(Vt_hat.T))

    [
        nu0,
        sigma20,
    ] = prior_info  # Since our prior is only in \sigma, we are letting the \betas be free beyond the simplex

    # From A_First_Course_in_Bayesian_Statistical_Methods (page ~ 159), nu0 represent the effective prior samples and sigma2_0 represents the expected prior variance

    cov_matrix_step = np.diag(S_hat**2 * stepsize**2)

    n = len(y)

    # Initializing the starting point at the average of all the models
    b_current = np.full(len(X.T), 0)

    supermodel_current = X.dot(b_current)

    residuals_current = y - supermodel_current

    log_likelihood_current = -np.sum(residuals_current**2)

    sigma2 = -log_likelihood_current / len(residuals_current)

    samples = []
    acceptance = 0

    for i in range(burn):
        # Sample from the conditional posterior of bs given sigma2 and data
        b_proposed = np.random.multivariate_normal(b_current, cov_matrix_step)

        # Tranform principal components' weights to models' weights
        omegas_proposed = np.dot(b_proposed, Vt_hat) + bias0

        # Comment this line and uncomment the next if you want to check MCMC without the simplex
        if np.any(omegas_proposed < 0):
            pass

        #         if 1>2:
        #             pass

        else:
            supermodel_proposed = X.dot(b_proposed)

            residuals_proposed = y - supermodel_proposed

            log_likelihood_proposed = -np.sum(residuals_proposed**2)

            #           Calculate the acceptance probability
            acceptance_prob = min(
                1,
                np.exp(
                    (+log_likelihood_proposed - log_likelihood_current)
                    / sigma2
                ),
            )

            # Accept or reject the proposal
            if np.random.uniform() < acceptance_prob:
                b_current = np.copy(b_proposed)
                log_likelihood_current = log_likelihood_proposed

        shape_post = (nu0 + n) / 2.0

        #         scale_post = (nu0*sigma20 - log_likelihood_current/len(residuals_current))/2.0

        scale_post = (nu0 * sigma20 - log_likelihood_current) / 2.0

        sigma2 = 1 / np.random.default_rng().gamma(shape_post, 1 / scale_post)

    #         samples.append(np.append(b_current,np.sqrt(sigma2)))

    for i in range(iterations):
        # Sample from the conditional posterior of bs given sigma2 and data
        b_proposed = np.random.multivariate_normal(b_current, cov_matrix_step)

        omegas_proposed = np.dot(b_proposed, Vt_hat) + bias0

        # Comment this line and uncomment the next if you want to check MCMC without the simplex
        if np.any(omegas_proposed < 0):
            pass

        #         if 1>2:
        #             pass
        else:
            supermodel_proposed = X.dot(b_proposed)

            residuals_proposed = y - supermodel_proposed

            log_likelihood_proposed = -np.sum(residuals_proposed**2)

            #           Calculate the acceptance probability
            acceptance_prob = min(
                1,
                np.exp(
                    (+log_likelihood_proposed - log_likelihood_current)
                    / sigma2
                ),
            )

            # Accept or reject the proposal
            if np.random.uniform() < acceptance_prob:
                b_current = np.copy(b_proposed)
                log_likelihood_current = log_likelihood_proposed
                acceptance = acceptance + 1

        shape_post = (nu0 + n) / 2.0

        scale_post = (nu0 * sigma20 - log_likelihood_current) / 2.0

        sigma2 = 1 / np.random.default_rng().gamma(shape_post, 1 / scale_post)

        samples.append(np.append(b_current, np.sqrt(sigma2)))

    print("we want percentage accepted to be around 20 to 40 percent")
    print(
        f"percentage actually accepted: {round(acceptance/iterations) * 100}%"
    )

    return np.array(samples)


def USVt_hat_extraction(U, S, Vt, components_kept):
    """
    Extract reduced-dimensionality matrices from Singular Value Decomposition (SVD).

    :param U: Left singular vectors from SVD.
    :param S: Singular values from SVD.
    :param Vt: Right singular vectors (transposed) from SVD.
    :param components_kept: Number of principal components to retain.
    :return: A tuple containing:
             - U_hat: Reduced left singular vectors.
             - S_hat: Retained singular values.
             - Vt_hat: Normalized right singular vectors.
             - Vt_hat_normalized: Original right singular vectors without normalization.
    """
    U_hat = np.array([U.T[i] for i in range(components_kept)]).T
    S_hat = S[:components_kept]
    Vt_hat = np.array([Vt[i] / S[i] for i in range(components_kept)])
    Vt_hat_normalized = np.array([Vt[i] for i in range(components_kept)])
    return U_hat, S_hat, Vt_hat, Vt_hat_normalized
