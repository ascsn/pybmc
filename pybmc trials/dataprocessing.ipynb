{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import corner \n",
    "from tqdm import tqdm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# #Some colors that Pablo likes:\n",
    "colors = [\n",
    "    \"#1f77b4\", # Vivid blue\n",
    "    \"#ff7f0e\", # Bright orange\n",
    "    \"#2ca02c\", # Rich green\n",
    "    \"#d62728\", # Strong red\n",
    "    \"#9467bd\", # Deep purple\n",
    "    # \"#8c564b\", # Brownish-pink\n",
    "    \"#e377c2\", # Pink\n",
    "    \"#7f7f7f\", # Medium gray\n",
    "    \"#bcbd22\", # Lime green\n",
    "    \"#17becf\", # Cyan\n",
    "    \"#393b79\", # Dark blue\n",
    "    \"#637939\", # Olive green\n",
    "    \"#8c6d31\", # Bronze\n",
    "    # \"#843c39\", # Dark red\n",
    "    # \"#ad494a\", # Reddish brown\n",
    "    \"#d6616b\", # Soft red\n",
    "    \"#e7ba52\", # Golden yellow\n",
    "    \"#7b4173\", # Dark purple\n",
    "    \"#a55194\", # Mauve\n",
    "    \"#ce6dbd\", # Light purple\n",
    "]\n",
    "\n",
    "colors_sets = [\n",
    "    \"#ff7f0e\",\n",
    "\n",
    "    \"#1f77b4\",\n",
    "\n",
    "    \"#2ca02c\",\n",
    "    \"#d62728\",\n",
    "    \n",
    "    \"#9467bd\",\n",
    "    \"#8c564b\",\n",
    "    \"#e377c2\",\n",
    "    \"#7f7f7f\",\n",
    "    \"#bcbd22\",\n",
    "    \"#17becf\",\n",
    "    '#1f77b4',  # muted blue\n",
    "    'r',  \n",
    "] \n",
    "\n",
    "markers = [\n",
    "    \"o\",  # Circle\n",
    "    \"^\",  # Triangle up\n",
    "    \"s\",  # Square\n",
    "    \"P\",  # Plus (filled)\n",
    "    \"*\",  # Star\n",
    "    \"X\",  # X (filled)\n",
    "    \"D\",  # Diamond\n",
    "    \"H\",  # Hexagon\n",
    "]\n",
    "\n",
    "\n",
    "colors_models = colors\n",
    "\n",
    "\n",
    "Selected_element=50\n",
    "Selected_element_name=\"Sn\"\n",
    "\n",
    "color_train=colors_sets[9]\n",
    "color_validation='orange'\n",
    "color_test=colors_sets[3]\n",
    "\n",
    "marker_train='s'\n",
    "marker_validation='*'\n",
    "marker_test='o'\n",
    "\n",
    "\n",
    "\n",
    "size_train=30\n",
    "size_validation=80\n",
    "size_test=35\n",
    "\n",
    "alpha_train=0.8\n",
    "alpha_validation=0.9\n",
    "alpha_test=0.4\n",
    "\n",
    "models = [ 'ME2', 'MEdelta', 'PC1', 'NL3S', 'SKMS', 'SKP', 'SLY4', 'SV', 'UNEDF0', \\\n",
    "        'UNEDF1']#Here I am copying all the models with Charge Raidus and Masses\n",
    "models_selected = [ 'ME2', 'MEdelta', 'PC1', 'NL3S', 'SKMS', 'SKP', 'SLY4', 'SV', 'UNEDF0', \\\n",
    "        'UNEDF1']#Here I am copying all the models with Charge Raidus and Masses\n",
    "key_list = models_selected\n",
    "\n",
    "heterogeneous_data_type = ['BE', 'ChRad', 'CPn', 'CPp', 'PEn', 'PEp', 'QDB2n', 'QDB2p', 'QDB4n', 'QDB4p', 'MRadN', 'MRadP']\n",
    "num_properties = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some functions that I use for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The functions that I define in here is basically the separting algorithm in which we have used\n",
    "for separting our dataset into training, validating, and testing region\n",
    "\"\"\"\n",
    "def separate_points_random(list1,random_chance):\n",
    "    \"\"\"\n",
    "    Separates points in list1 into two groups randomly\n",
    "\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    train_list_coordinates=[]\n",
    "    test_list_coordinates=[]\n",
    "\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        point1=list1[i]\n",
    "        val=np.random.rand()\n",
    "        if val<=random_chance:\n",
    "            train.append(point1)\n",
    "            train_list_coordinates.append(i)\n",
    "        else:\n",
    "            test.append(point1)\n",
    "            test_list_coordinates.append(i)\n",
    "\n",
    "    return np.array(train), np.array(test), np.array(train_list_coordinates), np.array(test_list_coordinates)\n",
    "\n",
    "def separate_points_distance(list1, list2, distance):\n",
    "    \"\"\"\n",
    "    Separates points in list1 into two groups based on their proximity to any point in list2.\n",
    "\n",
    "    :param list1: List of (x, y) tuples.\n",
    "    :param list2: List of (x, y) tuples.\n",
    "    :param distance: The threshold distance to determine proximity.\n",
    "    :return: Two lists - close_points and distant_points.\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    test = []\n",
    "\n",
    "    train_list_coordinates=[]\n",
    "    test_list_coordinates=[]\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        point1=list1[i]\n",
    "        close = False\n",
    "        for point2 in list2:\n",
    "            if np.linalg.norm(np.array(point1) - np.array(point2)) <= distance:\n",
    "                close = True\n",
    "                break\n",
    "        if close:\n",
    "            train.append(point1)\n",
    "            train_list_coordinates.append(i)\n",
    "        else:\n",
    "            test.append(point1)\n",
    "            test_list_coordinates.append(i)\n",
    "\n",
    "    return np.array(train), np.array(test), np.array(train_list_coordinates), np.array(test_list_coordinates)\n",
    "\n",
    "def separate_points_distance_allSets(list1, list2, distance1, distance2):\n",
    "    \"\"\"\n",
    "    Separates points in list1 into three groups based on their proximity to any point in list2.\n",
    "\n",
    "    :param list1: List of (x, y) tuples.\n",
    "    :param list2: List of (x, y) tuples.\n",
    "    :param distance: The threshold distance to determine proximity.\n",
    "    :return: Two lists - close_points and distant_points.\n",
    "    \"\"\"\n",
    "    train = []\n",
    "    validation=[]\n",
    "    test = []\n",
    "\n",
    "    train_list_coordinates=[]\n",
    "    validation_list_coordinates=[]\n",
    "    test_list_coordinates=[]\n",
    "\n",
    "    for i in range(len(list1)):\n",
    "        point1=list1[i]\n",
    "        close = False\n",
    "        for point2 in list2:\n",
    "            if np.linalg.norm(np.array(point1) - np.array(point2)) <= distance1:\n",
    "                close = True\n",
    "                break\n",
    "        if close:\n",
    "            train.append(point1)\n",
    "            train_list_coordinates.append(i)\n",
    "        else:\n",
    "            close2=False\n",
    "            for point2 in list2:\n",
    "                if np.linalg.norm(np.array(point1) - np.array(point2)) <= distance2:\n",
    "                    close2 = True\n",
    "                    break\n",
    "            if close2==True:\n",
    "                validation.append(point1)\n",
    "                validation_list_coordinates.append(i)\n",
    "            else:\n",
    "                test.append(point1)\n",
    "                test_list_coordinates.append(i)                \n",
    "\n",
    "    return np.array(train),np.array(validation), np.array(test), np.array(train_list_coordinates),  np.array(validation_list_coordinates),np.array(test_list_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NZ_synchronization(models_data_sets, models_selected):\n",
    "    \"\"\"\n",
    "    This function will helps synchronize our isotopes domain for all models that we choose to analyze. \n",
    "    models_data_sets will be the dataset of models' predictions without synchronized domain,\n",
    "    models_selected are models that we want to include, and filtered_NZ is just some initial list for \n",
    "    \"\"\"\n",
    "    # This line takes out isotopes of the first model and use that as the input for the selection step\n",
    "    filtered_NZ = np.array([models_data_sets[models_selected[0]]['N'].tolist(),\\\n",
    "                             models_data_sets[models_selected[0]]['Z'].tolist() ]).T\n",
    "\n",
    "    for model in models_selected:\n",
    "        filtered_NZ_new = []\n",
    "        for isotope in filtered_NZ:\n",
    "            if ( (isotope[0] == models_data_sets[model]['N']) & (isotope[1] == models_data_sets[model]['Z']) ).any(): # Choose nuclei that are contained in each model\n",
    "                filtered_NZ_new.append(isotope)\n",
    "        filtered_NZ = np.array(filtered_NZ_new) #update our new list of isotope and repeat this for every model\n",
    "    filtered_NZ_df = pd.DataFrame({'N' : filtered_NZ.T[0], 'Z' : filtered_NZ.T[1]})\n",
    "    return filtered_NZ_new\n",
    "\n",
    "def filtered_NZ_extraction(filtered_NZ):\n",
    "    \"\"\"\n",
    "    This will be the useful when we want to extract valid domain X for the model to work on. The code \n",
    "    that I am writing right now are selecting isotopes with proton and neutron number is bigger than 8 and\n",
    "    even. \n",
    "    \"\"\"\n",
    "    filtered_NZ_new = []\n",
    "    for isotope in filtered_NZ:\n",
    "        if ((isotope[0] >= 8) & (isotope[1] >=8)) & ((isotope[0]%2 == 0) & (isotope[1]%2 == 0)): # Sort the even-even isotope with proton and neutron number 8 or above\n",
    "            filtered_NZ_new.append(isotope)\n",
    "    filtered_NZ_new  = np.array(filtered_NZ_new)\n",
    "    return filtered_NZ_new\n",
    "\n",
    "def selected_models_data_sets_extraction(models_data_sets, models_selected, filtered_NZ_df, property):\n",
    "    \"\"\"\n",
    "    This helps create dataframe of models' predictions on filtered isotope\n",
    "    \"\"\"\n",
    "    selected_models_data_sets = pd.DataFrame(filtered_NZ_df) #Initiate the dataframe by the proton and neutron number\n",
    "    for model in models_selected:\n",
    "        merged_df = pd.merge(filtered_NZ_df, pd.DataFrame(models_data_sets[model]), on = ['N', 'Z'], how  = 'inner') # Merge the isotope with their corresponding predictions\n",
    "        selected_models_data_sets[model] = merged_df[property] # Choose the properties we want to perfrom BMM from\n",
    "    return selected_models_data_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "This cell helps you to extract a dictionary that contains all the nuclei with BE and charge\n",
    "radius predictions of all the models we have\"\n",
    "\"\"\"\n",
    "models_data_sets = {}\n",
    "for model in models:\n",
    "    Data_Values = pd.read_hdf(\"../data./selected_data.h5\", key = model)\n",
    "    models_data_sets[model] = {\"N\" : Data_Values[\"N\"], \"Z\" : Data_Values[\"Z\"], \\\n",
    "                               \"BE\" : Data_Values[\"BE\"], 'ChRad': Data_Values['ChRad']}\n",
    "\n",
    "\"\"\"\n",
    "This part extracts all the isotopes that exist in all models and satisfy our condition: In this case, \n",
    "N and Z is bigger than 8 and are even.\n",
    "\"\"\"\n",
    "merged_NZ = NZ_synchronization(models_data_sets, models_selected)\n",
    "filtered_NZ = filtered_NZ_extraction(merged_NZ)\n",
    "filtered_NZ_df = pd.DataFrame({'N' : filtered_NZ.T[0], 'Z' : filtered_NZ.T[1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the data that I store in the \"data\" folder on the \"Model0thorgonalization\" repo - the\n",
    "3-11-2025-check-point\n",
    "\n",
    "\"\"\"\n",
    "truth_CR_df = pd.read_csv(r'C:/Users/congn/OneDrive/Desktop/An Le Materials/ModelOrthogonalization/data/charge_radii.csv') # I download the radii data separately\n",
    "\"\"\"\n",
    "This is just to rename the columns so that I can merge data set later\n",
    "\"\"\"\n",
    "truth_CR_df.rename(columns = {'z' : 'Z', 'n' : 'N'}, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable isotopes is the basis that we use for separating our data into train, validation, test\n",
    "stable_coordinates_full=np.loadtxt(\"../Stable-Isotopes.txt\")\n",
    "# Make sure that nuclei we analyze have proton and neutron number of 8 or above\n",
    "stable_coordinates = filtered_NZ_extraction(stable_coordinates_full) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are dataframes of model's predictions only for charge radii and binding energies\n",
    "selected_models_data_sets_mass = selected_models_data_sets_extraction(models_data_sets, models_selected,\\\n",
    "                                                                       filtered_NZ_df, 'BE')\n",
    "selected_models_data_sets_radius = selected_models_data_sets_extraction(models_data_sets, models_selected,\\\n",
    "                                                                         filtered_NZ_df, 'ChRad')\n",
    "\n",
    "# This dataset does not yet contain the real data, let's now merge this dataset with the real data\n",
    "truth_BE = pd.read_hdf(\"../data./selected_data.h5\", key = 'AME2020')\n",
    "truth_BE_df = pd.DataFrame(truth_BE)\n",
    "# Add real data to the dataframe\n",
    "selected_models_data_sets_mass = pd.merge(selected_models_data_sets_mass, truth_BE[['N', 'Z', 'BE']],\\\n",
    "                                           on = ['N', 'Z'], how = 'inner')\n",
    "# Rename our truth column\n",
    "selected_models_data_sets_mass.rename(columns = {'BE' : 'truth'}, inplace = True)\n",
    "# Reorganizing the order of models' predictions\n",
    "cols_mass = list(selected_models_data_sets_mass.keys())\n",
    "cols_mass.remove('truth')\n",
    "cols_mass.insert(2, 'truth') # These 2 lines rearrange the order fo the column\n",
    "selected_models_data_sets_mass = selected_models_data_sets_mass[cols_mass]\n",
    "\n",
    "# After merging with the experimental data, we will want to redefine our isotope domain:\n",
    "filtered_NZ_df_mass = selected_models_data_sets_mass[['N', 'Z']]\n",
    "filtered_NZ_mass = np.array(filtered_NZ_df_mass)\n",
    "\n",
    "\n",
    "\n",
    "# We did the same for the the charge radii data\n",
    "selected_models_data_sets_radius = pd.merge(selected_models_data_sets_radius, truth_CR_df[['N',\\\n",
    "                                             'Z', 'radius_val']], how = 'inner', on = ['N', 'Z'])\n",
    "# Rename our experimental data\n",
    "selected_models_data_sets_radius.rename(columns = {'radius_val' : 'truth'}, inplace = True)\n",
    "# Reorganizing the order of models' predictions\n",
    "cols_radius = list(selected_models_data_sets_radius.keys())\n",
    "cols_radius.remove('truth')\n",
    "cols_radius.insert(2, 'truth')\n",
    "selected_models_data_sets_radius = selected_models_data_sets_radius[cols_radius]\n",
    "# We want to drop any NaN value from our dataset\n",
    "selected_models_data_sets_radius = selected_models_data_sets_radius.copy().dropna().reset_index(drop = True)\n",
    "\n",
    "# After merging with the experimental data, we will want to redefine our isotope domain:\n",
    "filtered_NZ_df_radius = selected_models_data_sets_radius[['N', 'Z']]\n",
    "filtered_NZ_radius = np.array(filtered_NZ_df_radius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After gettting the dataset that we want to work with, we will want to separate the data into training, validating, and testing sets\n",
    "This could be achieved with the separating algorithm function that I defined above. The code above extracts these data for charge radii\n",
    "\"\"\"\n",
    "\n",
    "distance1=1.75\n",
    "distance2=2.5\n",
    "\n",
    "training_set_radius, validation_set_radius, test_set_radius, train_coordinates_radius, validation_coordinates_radius,test_coordinates_radius=separate_points_distance_allSets(filtered_NZ_radius, stable_coordinates, distance1,distance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "After gettting the dataset that we want to work with, we will want to separate the data into training, validating, and testing sets\n",
    "This could be achieved with the separating algorithm function that I defined above. The code above extracts these data for mass\n",
    "\"\"\"\n",
    "\n",
    "distance1=2\n",
    "distance2=3\n",
    "\n",
    "training_set_mass, validation_set_mass, test_set_mass, train_coordinates_mass, validation_coordinates_mass,test_coordinates_mass=separate_points_distance_allSets(filtered_NZ_mass, stable_coordinates, distance1,distance2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas that I thought you could think for pybmc Dataset class\n",
    "+ What I want to achieve is to write an efficient class that can swiftly extract: a dataset that contains all models predictions extracted from the source, the isotopes that all exist in all models (or the domain X that the model mixing will work on), a function that extract subset of these domain (depends on the condition that we want to extract), the separation data algorithm that extract training, validaiton, and test datasets. In this notebook, I have separate functions that have all of these utilities, but they are still urdimentary functions that needs a lot of individual adjustment.\n",
    "+ A few problems with this: I have 2 data files that I extract on: the selected_data.h5 (contains all the models) and the charge_radii.csv (contains experimental data for charge radii). I do not know yet know to effectively write a function in the Dataset class that can extract both of these effectively, and then merge the experimental data with the models (I need to write a long ass code in in cell 7 to do this). I also do not know how to generalize the filtered_NZ_extraction (which also correspond to get_subset method) so that we can work with different conditions. Lastly, what I wrote in here only accounts for the case when we have nuclei's model, which is not generalizable for cases in which the users can extract their own models in and then perform these data-organization. \n",
    "+ You can start working on this and give me any ideas on how to fix this (even if you think that something are not necessary, or we should not approach the problem this way, please discuss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
