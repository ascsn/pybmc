{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyBMC Documentation Welcome to the official documentation for pyBMC, a Python package for Bayesian Model Combination (BMC) with a focus on nuclear mass predictions. Overview pyBMC provides a comprehensive framework for combining multiple predictive models using Bayesian statistics. Key features include: Data Management : Load and preprocess nuclear mass data from HDF5 and CSV files Orthogonalization : Transform model predictions using Singular Value Decomposition (SVD) Bayesian Inference : Perform Gibbs sampling for model combination Uncertainty Quantification : Generate predictions with credible intervals Model Evaluation : Calculate coverage statistics for model validation Getting Started Installation pip install pybmc Quick Start from pybmc import Dataset , BayesianModelCombination # Load nuclear mass data dataset = Dataset ( \"nuclear_data.h5\" ) data_dict = dataset . load_data ( models = [ \"FRDM2012\" , \"WS4\" , \"HFB32\" , \"D1M\" , \"UNEDF1\" , \"BCPM\" ], keys = [ \"Binding_Energy\" ], domain_keys = [ \"N\" , \"Z\" ] ) # Initialize BMC bmc = BayesianModelCombination ( models_list = [ \"FRDM2012\" , \"WS4\" , \"HFB32\" , \"D1M\" , \"UNEDF1\" , \"BCPM\" ], data_dict = data_dict , truth_column_name = \"Binding_Energy\" ) # Split data train_df , val_df , test_df = dataset . split_data ( data_dict , \"Binding_Energy\" , splitting_algorithm = \"random\" , train_size = 0.6 , val_size = 0.2 , test_size = 0.2 ) # Orthogonalize model predictions bmc . orthogonalize ( \"Binding_Energy\" , train_df , components_kept = 3 ) # Train the model combination bmc . train ( training_options = { 'iterations' : 50000 , 'sampler' : 'gibbs_sampling' }) # Make predictions rndm_m , lower_df , median_df , upper_df = bmc . predict2 ( \"Binding_Energy\" ) # Evaluate model performance coverage_results = bmc . evaluate () Documentation Contents Usage Guide : Detailed examples and tutorials API Reference : Complete documentation of all classes and functions Theory Background : Mathematical foundations of Bayesian model combination Contributing : How to contribute to pyBMC Support For questions or support, please open an issue on our GitHub repository . License This project is licensed under the GPL V3 License - see the LICENSE file for details.","title":"Home"},{"location":"#pybmc-documentation","text":"Welcome to the official documentation for pyBMC, a Python package for Bayesian Model Combination (BMC) with a focus on nuclear mass predictions.","title":"pyBMC Documentation"},{"location":"#overview","text":"pyBMC provides a comprehensive framework for combining multiple predictive models using Bayesian statistics. Key features include: Data Management : Load and preprocess nuclear mass data from HDF5 and CSV files Orthogonalization : Transform model predictions using Singular Value Decomposition (SVD) Bayesian Inference : Perform Gibbs sampling for model combination Uncertainty Quantification : Generate predictions with credible intervals Model Evaluation : Calculate coverage statistics for model validation","title":"Overview"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#installation","text":"pip install pybmc","title":"Installation"},{"location":"#quick-start","text":"from pybmc import Dataset , BayesianModelCombination # Load nuclear mass data dataset = Dataset ( \"nuclear_data.h5\" ) data_dict = dataset . load_data ( models = [ \"FRDM2012\" , \"WS4\" , \"HFB32\" , \"D1M\" , \"UNEDF1\" , \"BCPM\" ], keys = [ \"Binding_Energy\" ], domain_keys = [ \"N\" , \"Z\" ] ) # Initialize BMC bmc = BayesianModelCombination ( models_list = [ \"FRDM2012\" , \"WS4\" , \"HFB32\" , \"D1M\" , \"UNEDF1\" , \"BCPM\" ], data_dict = data_dict , truth_column_name = \"Binding_Energy\" ) # Split data train_df , val_df , test_df = dataset . split_data ( data_dict , \"Binding_Energy\" , splitting_algorithm = \"random\" , train_size = 0.6 , val_size = 0.2 , test_size = 0.2 ) # Orthogonalize model predictions bmc . orthogonalize ( \"Binding_Energy\" , train_df , components_kept = 3 ) # Train the model combination bmc . train ( training_options = { 'iterations' : 50000 , 'sampler' : 'gibbs_sampling' }) # Make predictions rndm_m , lower_df , median_df , upper_df = bmc . predict2 ( \"Binding_Energy\" ) # Evaluate model performance coverage_results = bmc . evaluate ()","title":"Quick Start"},{"location":"#documentation-contents","text":"Usage Guide : Detailed examples and tutorials API Reference : Complete documentation of all classes and functions Theory Background : Mathematical foundations of Bayesian model combination Contributing : How to contribute to pyBMC","title":"Documentation Contents"},{"location":"#support","text":"For questions or support, please open an issue on our GitHub repository .","title":"Support"},{"location":"#license","text":"This project is licensed under the GPL V3 License - see the LICENSE file for details.","title":"License"},{"location":"CONTRIBUTING/","text":"Contributing to pyBMC We welcome contributions from the community! This document outlines how you can contribute to the pyBMC project. Getting Started Fork the repository on GitHub Clone your fork locally: git clone https://github.com/your-username/pybmc.git cd pybmc Set up the development environment : poetry install Development Workflow Create a new branch for your feature or bug fix: git checkout -b feature/your-feature-name Make your changes and ensure tests pass: pytest Commit your changes with a descriptive message: git commit -m \"Add new feature for orthogonalization\" Push your branch to your fork: git push origin feature/your-feature-name Open a pull request against the main repository Coding Standards Follow PEP 8 style guidelines Use type hints for all function signatures Write docstrings for all public classes and functions using Google style Keep functions small and focused (under 50 lines when possible) Write unit tests for new features using pytest Documentation Update documentation in the docs/ directory Add examples for new features in usage.md Update API reference in api_reference.md when adding new public interfaces Testing Write tests for new features in the tests/ directory Ensure test coverage remains above 90% Run tests locally before submitting a PR: pytest --cov = pybmc Reporting Issues When reporting issues, please include: - Steps to reproduce the issue - Expected behavior - Actual behavior - Environment details (OS, Python version, etc) Code Review Process All pull requests require at least one maintainer approval Maintainers will review for: Code quality and style Test coverage Documentation updates Backward compatibility Be prepared to make revisions based on feedback License By contributing to pyBMC, you agree that your contributions will be licensed under the MIT License.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-to-pybmc","text":"We welcome contributions from the community! This document outlines how you can contribute to the pyBMC project.","title":"Contributing to pyBMC"},{"location":"CONTRIBUTING/#getting-started","text":"Fork the repository on GitHub Clone your fork locally: git clone https://github.com/your-username/pybmc.git cd pybmc Set up the development environment : poetry install","title":"Getting Started"},{"location":"CONTRIBUTING/#development-workflow","text":"Create a new branch for your feature or bug fix: git checkout -b feature/your-feature-name Make your changes and ensure tests pass: pytest Commit your changes with a descriptive message: git commit -m \"Add new feature for orthogonalization\" Push your branch to your fork: git push origin feature/your-feature-name Open a pull request against the main repository","title":"Development Workflow"},{"location":"CONTRIBUTING/#coding-standards","text":"Follow PEP 8 style guidelines Use type hints for all function signatures Write docstrings for all public classes and functions using Google style Keep functions small and focused (under 50 lines when possible) Write unit tests for new features using pytest","title":"Coding Standards"},{"location":"CONTRIBUTING/#documentation","text":"Update documentation in the docs/ directory Add examples for new features in usage.md Update API reference in api_reference.md when adding new public interfaces","title":"Documentation"},{"location":"CONTRIBUTING/#testing","text":"Write tests for new features in the tests/ directory Ensure test coverage remains above 90% Run tests locally before submitting a PR: pytest --cov = pybmc","title":"Testing"},{"location":"CONTRIBUTING/#reporting-issues","text":"When reporting issues, please include: - Steps to reproduce the issue - Expected behavior - Actual behavior - Environment details (OS, Python version, etc)","title":"Reporting Issues"},{"location":"CONTRIBUTING/#code-review-process","text":"All pull requests require at least one maintainer approval Maintainers will review for: Code quality and style Test coverage Documentation updates Backward compatibility Be prepared to make revisions based on feedback","title":"Code Review Process"},{"location":"CONTRIBUTING/#license","text":"By contributing to pyBMC, you agree that your contributions will be licensed under the MIT License.","title":"License"},{"location":"api_reference/","text":"API Reference pybmc.data Dataset class Dataset : \"\"\" Handles loading and preprocessing of nuclear mass data. Args: data_source (str): Path to data file (HDF5 or CSV) Methods: load_data(models, keys, domain_keys): Load data for specified models split_data(data_dict, target_key, splitting_algorithm, **kwargs): Split data into train/val/test sets \"\"\" pybmc.bmc BayesianModelCombination class BayesianModelCombination : \"\"\" Main class for Bayesian Model Combination. Args: models_list (list): List of model names data_dict (dict): Dictionary containing model predictions truth_column_name (str): Name of column containing true values Methods: orthogonalize(target_key, train_df, components_kept): Perform SVD orthogonalization train(training_options): Train the model combination predict2(target_key): Generate predictions with uncertainty evaluate(): Calculate coverage statistics \"\"\" pybmc.inference_utils gibbs_sampler def gibbs_sampler ( y , X , iterations , prior_info ): \"\"\" Performs Gibbs sampling for Bayesian linear regression. Args: y (np.ndarray): Response vector (centered) X (np.ndarray): Design matrix iterations (int): Number of sampling iterations prior_info (tuple): Prior parameters Returns: np.ndarray: Posterior samples [beta, sigma] \"\"\" gibbs_sampler_simplex def gibbs_sampler_simplex ( y , X , Vt_hat , S_hat , iterations , prior_info , burn = 10000 , stepsize = 0.001 ): \"\"\" Performs Gibbs sampling with simplex constraints on model weights. Args: y (np.ndarray): Centered response vector X (np.ndarray): Design matrix of principal components Vt_hat (np.ndarray): Normalized right singular vectors S_hat (np.ndarray): Singular values iterations (int): Number of sampling iterations prior_info (list): [nu0, sigma20] - prior parameters for variance burn (int, optional): Burn-in iterations (default: 10000) stepsize (float, optional): Proposal step size (default: 0.001) Returns: np.ndarray: Posterior samples [beta, sigma] \"\"\" USVt_hat_extraction def USVt_hat_extraction ( U , S , Vt , components_kept ): \"\"\" Extracts reduced-dimensionality matrices from SVD results. Args: U (np.ndarray): Left singular vectors S (np.ndarray): Singular values Vt (np.ndarray): Right singular vectors (transposed) components_kept (int): Number of components to retain Returns: tuple: (U_hat, S_hat, Vt_hat, Vt_hat_normalized) \"\"\" pybmc.sampling_utils coverage def coverage ( percentiles , rndm_m , models_output , truth_column ): \"\"\" Calculates coverage percentages for credible intervals. Args: percentiles (list): Percentiles to evaluate rndm_m (np.ndarray): Posterior samples of predictions models_output (pd.DataFrame): DataFrame containing true values truth_column (str): Name of column with true values Returns: list: Coverage percentages for each percentile \"\"\" rndm_m_random_calculator ```python def rndm_m_random_calculator(filtered_model_predictions, samples, Vt_hat): \"\"\" Generates posterior predictive samples and credible intervals. Args: filtered_model_predictions (np.ndarray): Model predictions samples (np.ndarray): Gibbs samples [beta, sigma] Vt_hat (np.ndarray): Normalized right singular vectors Returns: tuple: - rndm_m (np.ndarray): Posterior predictive samples - [lower, median, upper] (list): Credible interval arrays \"\"\"","title":"API Reference"},{"location":"api_reference/#api-reference","text":"","title":"API Reference"},{"location":"api_reference/#pybmcdata","text":"","title":"pybmc.data"},{"location":"api_reference/#dataset","text":"class Dataset : \"\"\" Handles loading and preprocessing of nuclear mass data. Args: data_source (str): Path to data file (HDF5 or CSV) Methods: load_data(models, keys, domain_keys): Load data for specified models split_data(data_dict, target_key, splitting_algorithm, **kwargs): Split data into train/val/test sets \"\"\"","title":"Dataset"},{"location":"api_reference/#pybmcbmc","text":"","title":"pybmc.bmc"},{"location":"api_reference/#bayesianmodelcombination","text":"class BayesianModelCombination : \"\"\" Main class for Bayesian Model Combination. Args: models_list (list): List of model names data_dict (dict): Dictionary containing model predictions truth_column_name (str): Name of column containing true values Methods: orthogonalize(target_key, train_df, components_kept): Perform SVD orthogonalization train(training_options): Train the model combination predict2(target_key): Generate predictions with uncertainty evaluate(): Calculate coverage statistics \"\"\"","title":"BayesianModelCombination"},{"location":"api_reference/#pybmcinference_utils","text":"","title":"pybmc.inference_utils"},{"location":"api_reference/#gibbs_sampler","text":"def gibbs_sampler ( y , X , iterations , prior_info ): \"\"\" Performs Gibbs sampling for Bayesian linear regression. Args: y (np.ndarray): Response vector (centered) X (np.ndarray): Design matrix iterations (int): Number of sampling iterations prior_info (tuple): Prior parameters Returns: np.ndarray: Posterior samples [beta, sigma] \"\"\"","title":"gibbs_sampler"},{"location":"api_reference/#gibbs_sampler_simplex","text":"def gibbs_sampler_simplex ( y , X , Vt_hat , S_hat , iterations , prior_info , burn = 10000 , stepsize = 0.001 ): \"\"\" Performs Gibbs sampling with simplex constraints on model weights. Args: y (np.ndarray): Centered response vector X (np.ndarray): Design matrix of principal components Vt_hat (np.ndarray): Normalized right singular vectors S_hat (np.ndarray): Singular values iterations (int): Number of sampling iterations prior_info (list): [nu0, sigma20] - prior parameters for variance burn (int, optional): Burn-in iterations (default: 10000) stepsize (float, optional): Proposal step size (default: 0.001) Returns: np.ndarray: Posterior samples [beta, sigma] \"\"\"","title":"gibbs_sampler_simplex"},{"location":"api_reference/#usvt_hat_extraction","text":"def USVt_hat_extraction ( U , S , Vt , components_kept ): \"\"\" Extracts reduced-dimensionality matrices from SVD results. Args: U (np.ndarray): Left singular vectors S (np.ndarray): Singular values Vt (np.ndarray): Right singular vectors (transposed) components_kept (int): Number of components to retain Returns: tuple: (U_hat, S_hat, Vt_hat, Vt_hat_normalized) \"\"\"","title":"USVt_hat_extraction"},{"location":"api_reference/#pybmcsampling_utils","text":"","title":"pybmc.sampling_utils"},{"location":"api_reference/#coverage","text":"def coverage ( percentiles , rndm_m , models_output , truth_column ): \"\"\" Calculates coverage percentages for credible intervals. Args: percentiles (list): Percentiles to evaluate rndm_m (np.ndarray): Posterior samples of predictions models_output (pd.DataFrame): DataFrame containing true values truth_column (str): Name of column with true values Returns: list: Coverage percentages for each percentile \"\"\"","title":"coverage"},{"location":"api_reference/#rndm_m_random_calculator","text":"```python def rndm_m_random_calculator(filtered_model_predictions, samples, Vt_hat): \"\"\" Generates posterior predictive samples and credible intervals. Args: filtered_model_predictions (np.ndarray): Model predictions samples (np.ndarray): Gibbs samples [beta, sigma] Vt_hat (np.ndarray): Normalized right singular vectors Returns: tuple: - rndm_m (np.ndarray): Posterior predictive samples - [lower, median, upper] (list): Credible interval arrays \"\"\"","title":"rndm_m_random_calculator"},{"location":"theory/","text":"Theoretical Background This section will provide a short explanation of the Bayesian Model Combination (BMC) methodology used in pyBMC. Bayesian Model Combination Bayesian Model Combination (BMC) is a method for combining multiple predictive models in a Bayesian framework. The key idea is to treat the model combination weights as random variables and infer their posterior distribution given the data. Mathematical Formulation Given a set of models \\( M_1, M_2, \\dots, M_K \\) , the combined prediction for a data point \\( x \\) is: \\[ y = \\sum_{k=1}^K w_k f_k(x) \\] where: - \\( f_k(x) \\) is the prediction of model \\( M_k \\) for input \\( x \\) - \\( w_k \\) is the weight assigned to model \\( M_k \\) , with \\( \\sum_{k=1}^K w_k = 1 \\) and \\( w_k \\geq 0 \\) Bayesian Inference We place a prior distribution on the weights \\( w \\) and update this prior using observed data to obtain the posterior distribution: \\[ p(w | D) \\propto p(D | w) p(w) \\] where \\( D \\) is the observed data. Gibbs Sampling We use Gibbs sampling to approximate the posterior distribution of the weights. The Gibbs sampler iteratively samples each weight conditional on the current values of the other weights and the data. Orthogonalization To address collinearity between model predictions, we perform an orthogonalization step using Singular Value Decomposition (SVD). This transforms the model predictions into a set of orthogonal basis vectors, which improves the stability of the Bayesian inference and prevents overfitting.","title":"Theory"},{"location":"theory/#theoretical-background","text":"This section will provide a short explanation of the Bayesian Model Combination (BMC) methodology used in pyBMC.","title":"Theoretical Background"},{"location":"theory/#bayesian-model-combination","text":"Bayesian Model Combination (BMC) is a method for combining multiple predictive models in a Bayesian framework. The key idea is to treat the model combination weights as random variables and infer their posterior distribution given the data.","title":"Bayesian Model Combination"},{"location":"theory/#mathematical-formulation","text":"Given a set of models \\( M_1, M_2, \\dots, M_K \\) , the combined prediction for a data point \\( x \\) is: \\[ y = \\sum_{k=1}^K w_k f_k(x) \\] where: - \\( f_k(x) \\) is the prediction of model \\( M_k \\) for input \\( x \\) - \\( w_k \\) is the weight assigned to model \\( M_k \\) , with \\( \\sum_{k=1}^K w_k = 1 \\) and \\( w_k \\geq 0 \\)","title":"Mathematical Formulation"},{"location":"theory/#bayesian-inference","text":"We place a prior distribution on the weights \\( w \\) and update this prior using observed data to obtain the posterior distribution: \\[ p(w | D) \\propto p(D | w) p(w) \\] where \\( D \\) is the observed data.","title":"Bayesian Inference"},{"location":"theory/#gibbs-sampling","text":"We use Gibbs sampling to approximate the posterior distribution of the weights. The Gibbs sampler iteratively samples each weight conditional on the current values of the other weights and the data.","title":"Gibbs Sampling"},{"location":"theory/#orthogonalization","text":"To address collinearity between model predictions, we perform an orthogonalization step using Singular Value Decomposition (SVD). This transforms the model predictions into a set of orthogonal basis vectors, which improves the stability of the Bayesian inference and prevents overfitting.","title":"Orthogonalization"},{"location":"usage/","text":"Usage Here is an example of how to use the package: import numpy as np from pybmc.models import Model from pybmc.data import Dataset from pybmc.bmc import BayesianModelCombination # Create models model1 = Model ( \"model1\" , np . array ([ 1 , 2 , 3 ]), np . array ([ 10 , 20 , 30 ])) model2 = Model ( \"model2\" , np . array ([ 1 , 2 , 3 ]), np . array ([ 15 , 25 , 35 ])) # Load data data_source = \"path/to/data_source\" dataset = Dataset ( data_source ) data = dataset . load_data ( data_source ) # Split data train_data , val_data , test_data = dataset . split_data ( train_size = 0.6 , val_size = 0.2 , test_size = 0.2 ) # Create Bayesian model combination bmc = BayesianModelCombination ( models = [ model1 , model2 ], options = { 'use_orthogonalization' : True }) # Orthogonalize models (optional) bmc . orthogonalize ( train_data ) # Train the model combination bmc . train ( train_data ) # Predict using the model combination X = np . array ([ 1 , 2 , 3 ]) predictions = bmc . predict ( X ) # Evaluate the model combination evaluation = bmc . evaluate ( val_data ) Explanation of Each Step Create models : We create two instances of the Model class, model1 and model2 , with their respective domains and outputs. Load data : We create an instance of the Dataset class and load data from a specified source. Split data : We split the loaded data into training, validation, and testing sets. Create Bayesian model combination : We create an instance of the BayesianModelCombination class with the created models and an option to use orthogonalization. Orthogonalize models (optional) : We orthogonalize the models using the training data if the orthogonalization option is enabled. Train the model combination : We train the Bayesian model combination using the training data. Predict using the model combination : We use the trained model combination to make predictions for a given input. Evaluate the model combination : We evaluate the performance of the model combination using the validation data.","title":"Usage"},{"location":"usage/#usage","text":"Here is an example of how to use the package: import numpy as np from pybmc.models import Model from pybmc.data import Dataset from pybmc.bmc import BayesianModelCombination # Create models model1 = Model ( \"model1\" , np . array ([ 1 , 2 , 3 ]), np . array ([ 10 , 20 , 30 ])) model2 = Model ( \"model2\" , np . array ([ 1 , 2 , 3 ]), np . array ([ 15 , 25 , 35 ])) # Load data data_source = \"path/to/data_source\" dataset = Dataset ( data_source ) data = dataset . load_data ( data_source ) # Split data train_data , val_data , test_data = dataset . split_data ( train_size = 0.6 , val_size = 0.2 , test_size = 0.2 ) # Create Bayesian model combination bmc = BayesianModelCombination ( models = [ model1 , model2 ], options = { 'use_orthogonalization' : True }) # Orthogonalize models (optional) bmc . orthogonalize ( train_data ) # Train the model combination bmc . train ( train_data ) # Predict using the model combination X = np . array ([ 1 , 2 , 3 ]) predictions = bmc . predict ( X ) # Evaluate the model combination evaluation = bmc . evaluate ( val_data )","title":"Usage"},{"location":"usage/#explanation-of-each-step","text":"Create models : We create two instances of the Model class, model1 and model2 , with their respective domains and outputs. Load data : We create an instance of the Dataset class and load data from a specified source. Split data : We split the loaded data into training, validation, and testing sets. Create Bayesian model combination : We create an instance of the BayesianModelCombination class with the created models and an option to use orthogonalization. Orthogonalize models (optional) : We orthogonalize the models using the training data if the orthogonalization option is enabled. Train the model combination : We train the Bayesian model combination using the training data. Predict using the model combination : We use the trained model combination to make predictions for a given input. Evaluate the model combination : We evaluate the performance of the model combination using the validation data.","title":"Explanation of Each Step"}]}